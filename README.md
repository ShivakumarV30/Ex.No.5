# **EXPERIMENT – 5: Comparative Analysis of Different Types of Prompting Patterns and Explanation with Various Test Scenarios**

**Register No.:** 212222060239
**Date:** 03.11.2025  

---

## **Aim**
To systematically evaluate and compare the effectiveness of different prompting patterns — such as **Zero-shot**, **Few-shot**, **Chain-of-Thought**, **Role-based**, and **ReAct prompting** — by testing them across diverse question types.  
The goal is to analyze the **quality**, **accuracy**, and **depth** of responses generated by multiple AI tools (**ChatGPT**, **Gemini**, **Claude**, and **Perplexity**) and determine how different prompt styles influence the outcome.

---

## **AI Tools Used**
- **ChatGPT (OpenAI)**  
- **Gemini (Google AI)**  
- **Claude (Anthropic)**  
- **Perplexity AI**

---

## **Theory**
Prompt engineering techniques define how effectively an AI model interprets and generates answers.  
Different prompting styles affect the **context understanding**, **depth of reasoning**, and **accuracy** of the responses.

| Prompt Type | Description | Typical Use Case |
|--------------|--------------|------------------|
| **Zero-shot** | Model answers directly with no examples or prior context. | Factual / definition questions |
| **Few-shot** | A few examples are provided before asking the main question. | Classification / comparison tasks |
| **Chain-of-Thought** | The AI is encouraged to show step-by-step reasoning. | Analytical / logical problems |
| **Role-based** | The model is given a specific role or perspective. | Professional or reflective prompts |
| **ReAct Prompting** | Combines reasoning with action or decision-making. | Analytical or decision-driven scenarios |

---

## **Procedure**
1. Select a variety of questions representing factual, analytical, creative, technical, and reflective categories.  
2. Use different **prompting patterns** (Zero-shot, Few-shot, Chain-of-thought, etc.) for each question.  
3. Obtain responses from **ChatGPT**, **Gemini**, **Claude**, and **Perplexity**.  
4. Record the content, structure, and style of the responses.  
5. Analyze the quality of answers using parameters — **clarity**, **accuracy**, **depth**, and **context relevance**.  
6. Compare how each AI behaves across different prompt types.

---

## **Detailed Test Scenarios**

### **Question 1: What is the Great Barrier Reef?**
**Prompt Type:** Zero-shot  
**Type:** Factual / Descriptive  

#### **ChatGPT**
The Great Barrier Reef is the world's largest coral reef system, located off the northeast coast of Queensland, Australia. Stretching over 2,300 kilometers, it is composed of more than 2,900 individual reefs and 900 islands. The reef is home to thousands of marine species and is a UNESCO World Heritage Site. However, it faces threats from coral bleaching, climate change, pollution, and overfishing.

#### **Gemini**
The Great Barrier Reef is a massive coral ecosystem along Australia’s Queensland coast. It includes over 2,900 reefs and 900 islands spanning more than 2,300 km. It is known for its biodiversity and is under threat from ocean warming and pollution.

#### **Claude**
The Great Barrier Reef is the world’s largest coral reef system off Queensland, Australia. It stretches 2,300 km, consists of nearly 3,000 reefs, and supports diverse life forms like corals, fish, and turtles. It’s a UNESCO site visible from space, but faces major ecological threats due to human activities.

#### **Perplexity**
The Great Barrier Reef spans over 2,300 km along the Queensland coast and includes 400+ coral species and 1,500+ fish species. It was designated a UNESCO World Heritage Site in 1981 and is visible from space.

**Analysis:**  
All four models provided accurate facts. **ChatGPT** and **Claude** gave detailed contextual information, **Gemini** was concise, and **Perplexity** offered statistical precision.  
**Prompting Pattern:** Zero-shot Prompting  

---

### **Question 2: Why did the Roman Empire fall?**
**Prompt Type:** Chain-of-Thought  
**Type:** Analytical / Historical  

#### **ChatGPT**
The Roman Empire fell due to internal and external factors. Internally, economic decline, political corruption, overreliance on slavery, and moral decay weakened governance. Externally, invasions by Visigoths, Vandals, and Huns caused collapse. The Western Empire officially ended in 476 AD when Romulus Augustulus was deposed, while the Eastern Empire (Byzantine) continued until 1453 AD.

#### **Gemini**
The Roman Empire’s fall was caused by weak leadership, inflation, civil wars, and repeated invasions. Dividing the empire into East and West reduced unity, and constant warfare drained its economy.

#### **Claude**
Internal corruption, economic inflation, and overexpansion eroded Rome’s stability. Military weaknesses and invasions by Germanic tribes culminated in the sack of Rome (410 CE). The Western Empire fell in 476 CE; the Eastern survived for centuries.

#### **Perplexity**
The fall of Rome was a multi-century decline driven by political instability, economic issues, and invasions from Germanic tribes. Overreliance on mercenaries and empire division in 395 CE worsened collapse. The Western Empire ended in 476 CE.

**Analysis:**  
**ChatGPT** and **Claude** demonstrated strong logical structuring and multi-factor reasoning. **Gemini** was concise, while **Perplexity** was data-rich and timeline-oriented.  
**Prompting Pattern:** Chain-of-Thought  

---

### **Question 3: Is 97 a prime number?**
**Prompt Type:** Chain-of-Thought  
**Type:** Mathematical / Logical  

#### **ChatGPT**
Yes, 97 is a prime number. It’s only divisible by 1 and itself. Testing divisibility by smaller primes (2, 3, 5, 7) confirms it cannot be divided evenly.  

#### **Gemini**
97 is prime because it has no divisors other than 1 and 97.  

#### **Claude**
A prime number has two factors: 1 and itself. Since 97 meets this rule, it’s prime.  

#### **Perplexity**
Testing divisibility up to √97 (≈9.85) shows no exact divisor; therefore, 97 is prime.

**Analysis:**  
All models gave correct answers. **ChatGPT** and **Perplexity** displayed reasoning steps, making them more reliable.  
**Prompting Pattern:** Chain-of-Thought  

---

### **Question 4: Write a Python function to reverse a string.**
**Prompt Type:** Zero-shot + Instruction  
**Type:** Technical / Code Generation  

#### **ChatGPT**
```python
def reverse_string(s):
    return s[::-1]
```
**Explanation:** Uses slicing to reverse the string efficiently.

#### **Gemini**
```python
def reverse_string(text):
    return text[::-1]
```

**Alternate:**
```python
def reverse_loop(text):
    result = ""
    for ch in text:
        result = ch + result
    return result
```
**Explanation:** Provides both slicing and iterative methods for better understanding.

#### **Claude**
```python
def reverse_string(s):
    return ''.join(reversed(s))
```
**Explanation:** Uses Python’s built-in `reversed()` function for clarity and efficiency.

#### **Perplexity**
```python
def rev(s):
    return s[::-1]
```
**Explanation:** Minimal but correct code implementation.

**Analysis:**  
All codes are correct. Gemini and ChatGPT provided alternate methods; Claude emphasized clarity; Perplexity was minimal.  
**Prompting Pattern:** Zero-shot + Instruction  

---

### **Question 5: Write a short poem about stars.**
**Prompt Type:** Zero-shot  
**Type:** Creative / Generative  

#### **ChatGPT**
> Stars whisper secrets in the sky,  
> Glowing softly as dreams drift by.  
> Through endless night their stories gleam,  
> Lighting hearts with silver beams.  

#### **Gemini**
> Across the silent midnight sea,  
> Stars sing of time and mystery.  
> Each spark a memory burning bright,  
> A mirror of our inner light.  

#### **Claude**
> The stars awake where silence grows,  
> Their shimmer paints the night’s repose.  
> A thousand fires in velvet skies,  
> Reflect the hope in human eyes.  

#### **Perplexity**
Stars are bright points of light in the night sky, formed by gases burning at high temperatures.

**Analysis:**  
Claude and ChatGPT generated poetic rhythm and emotion. Gemini emphasized imagery, while Perplexity lacked creativity.  
**Prompting Pattern:** Zero-shot  

---

### **Question 6: Is it cheaper to take a bus or a train from Chennai to Bangalore?**
**Prompt Type:** Few-shot  
**Type:** Practical / Cost Comparison  

#### **ChatGPT**
Bus fares range ₹350–₹800; trains ₹160–₹700 depending on class. Trains are usually cheaper when booked early, but buses are flexible and faster for late bookings.

#### **Gemini**
Government buses cost ₹400–₹700; private ₹500–₹1,200. Trains range ₹200–₹800. Trains are cheaper overall but buses offer more schedules.

#### **Claude**
Bus: ₹400–₹1,000; Train: ₹200–₹700. Trains are cost-effective; buses offer convenience. Comparison depends on travel comfort and timing.

#### **Perplexity**
Not enough public fare data available for a direct comparison.

**Analysis:**  
ChatGPT and Gemini gave realistic comparative answers; Claude was analytical; Perplexity lacked cost context.  
**Prompting Pattern:** Few-shot  

---

### **Question 7: Is happiness a choice?**
**Prompt Type:** Role-based / Reflective  
**Type:** Philosophical / Opinion-based  

#### **ChatGPT**
Happiness is partly a choice. People can choose gratitude and positive responses, but external factors—health, society, relationships—also affect well-being. It’s both mindset and circumstance.

#### **Gemini**
Happiness depends on both perspective and situation. Practices like mindfulness increase happiness, yet genetics and environment play roles.

#### **Claude**
Happiness involves voluntary and involuntary elements. Psychology shows intentional activities—gratitude, compassion—boost joy, but genetics, trauma, and social conditions limit full control. Philosophy views happiness as balance between control and acceptance.

#### **Perplexity**
Insufficient data; subjective question with no direct factual answer.

**Analysis:**  
Claude provided the most insightful, nuanced response; ChatGPT balanced emotional and rational elements; Gemini stayed concise.  
**Prompting Pattern:** Role-based / Reflective  

---

### **Question 8: Compare Wi-Fi and Bluetooth.**
**Prompt Type:** Direct Comparison  
**Type:** Technical / Analytical  

#### **ChatGPT**
| Feature | Wi-Fi | Bluetooth |
|----------|--------|------------|
| **Range** | Up to 100 m | Around 10 m |
| **Speed** | Up to 9.6 Gbps (Wi-Fi 6) | Up to 50 Mbps |
| **Power** | Higher | Low |
| **Use Case** | Internet access, networking | Short-range device connection |

#### **Gemini**
Wi-Fi is designed for long-range high-speed networking; Bluetooth is optimized for short-range, low-energy communication.

#### **Claude**
Wi-Fi covers larger areas with higher data speeds but consumes more power. Bluetooth offers short-range, low-energy connections ideal for personal devices. Both use encryption but differ in scope.

#### **Perplexity**
Provided a short technical description of both, lacking comparison table.

**Analysis:**  
ChatGPT gave the clearest tabular comparison. Claude and Gemini offered structured overviews; Perplexity lacked detailed formatting.  
**Prompting Pattern:** Direct / Analytical  

---

## **Overall Evaluation Summary**

| AI Tool | Strengths | Weaknesses | Best Performing Prompt Type |
|----------|------------|-------------|-------------------------------|
| **ChatGPT** | Balanced, detailed, and contextually adaptive answers. | Occasionally verbose. | Chain-of-Thought, Zero-shot |
| **Gemini** | Concise, accurate, strong in technical and factual tasks. | Limited emotional depth. | Few-shot, Technical |
| **Claude** | Deep analytical reasoning and structured writing. | Slower, lengthy responses. | Role-based, Analytical |
| **Perplexity** | High factual reliability and data accuracy. | Limited creativity and style. | Chain-of-Thought, Factual |

---

### **Analysis**
- **ChatGPT:** Best for well-rounded and contextual responses.  
- **Gemini:** Ideal for concise technical writing.  
- **Claude:** Excels at detailed philosophical and analytical discussions.  
- **Perplexity:** Best for factual precision and reference-based information.  

Prompt clarity and structure consistently improve accuracy and depth across all AI tools.

---

### **Conclusion**
Different prompting patterns produce distinct behaviors in AI systems:  
- **Zero-shot** works well for factual questions.  
- **Few-shot** improves reasoning with examples.  
- **Chain-of-Thought** enhances analytical tasks.  
- **Role-based** adds depth and perspective.  
- **ReAct** prompts (not shown here) integrate reasoning and decision flow for complex contexts.  

Overall, **ChatGPT** demonstrated the most balanced capability, **Claude** provided the deepest insights, **Gemini** was the most concise, and **Perplexity** offered the most factual precision.

---

### **Output**
All test scenarios were successfully executed using ChatGPT, Gemini, Claude, and Perplexity AI under different prompting patterns.

---

### **Result**
The various types of prompting patterns were executed successfully, and the detailed comparative report was generated.
